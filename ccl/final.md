# TextCNN 文本分类

## 摘要
数据挖掘课程大作业，主要使用了PyTorch实现了TextCNN，对TNEWS数据集进行新闻分类工作。TextCNN网络结构主要为嵌入层、卷积层、max池化层、全连接层组成。在这次大作业中，老师已经基本写好了网络的训练部分，因此在训练部分主要是通过单步调试程序， 对TextCNN的训练过程进行学习。在了解了TextCNN的工作流程后，自己简单的写了一个测试模块，并且最终输出classification_report代表模型的整体效果的评价。


## 1. 数据预处理
程序中对数据集构建了三个类，最外层是DataBatchIterator，里面包含DataSet类和Vocab类
DataSet类：即真正地存储数据的类，包含读取数据的方法和相关数值化处理。
Vocab类：字词和标签对数值的映射表类，内含{字词：数值}，{数值：字词}字典，以及字词的频率统计字典。

读取数据时，DataSet类按行读取数据集中的词组和标签。而DataBatchIterator类利用DataSet数据，构建Vocab。其中因为min_freq的设置，筛选出所有频率大于5的字词，并且按出现的先后顺序构建对数值的映射关系。标签映射表的制作也是同理。

获取映射表后，即可将原DataSet数据进行数值化，将字词和标签都转换成整数表示，此时我们拥有了数字表示的数据集。
## 2. 网络训练过程
训练中数据划分为多个batch，每个batch大小为32，即每次每批根据32条数据进行训练。

输入层和嵌入层：
输入层本质上就是二维矩阵表示的数据集，只不过数据不能用原有的字符形式进行表示，要转换为网络可用的数值向量形式。换句话说其实就是对原有输入数据中的每个词转换成了一个数值。例如词有3000个， 则每个词由0~2999中的一个代表。则一个由5个词组成的句子，则是5X3000的矩阵。当然我们知道整个语料范围内词的数量是特别多的，因此这样表示的矩阵会非常庞大。因此利用一个嵌入层，通过设定好嵌入层的深度，将3000维表示的词投影到低维度中，例如这次大作业中是256维，即进行降维处理，有点类似PCA。
当然这里使用是数据词向量是自己制作的，但我查找资料得知其实可以导入别人预训练好的词向量，并且优质的词向量往往可以带来更好的结果。


卷积层：卷积是一种常用的数学运算， 它将卷积核和其覆盖范围中的数据进行对应相乘，然后累加求和，最终将一块矩阵区域的信息提取为一个输出值。因此算是一种数据特征提取的方式。而对于文本处理，我们传给卷积层的数据中每一行代表的是一个字词的向量编码，行数为字词数。即每一整行才拥有这个字的完整信息，所以对行长度上的切割是没有意义的。因此TextCNN卷积核的宽度和词的向量宽度是一样的，而高度一般是（2，3，4）。即通过卷积提取连续2，3，4个词的特征，这样卷积结果不仅拥有每个词的信息，还一定程度上代表着上下文词间关系。

池化层：因为每个卷积核的高度不一样，因此要归纳所有卷积信息，需要将每个卷积核的信息进行采样成统一的表达方式，一般是使用采样卷积结果中的最大值。最后将归纳的特征信息传递给下一层进行处理。

全连接层：将获得的上一层的所有特征信息，映射到标签域中，实现神经网络的输出。与卷积层相比较，卷积层是抽取局部特征出来，而全连接层是综合观察所有的局部特征，再次组合成全局整体特征。

全连接层输出为logits结果，

## 3. 测试结果



## 参考文献
文本分类算法TextCNN原理详解（一） https://www.cnblogs.com/ModifyRong/p/11319301.html
神经网络 优化器 https://www.jianshu.com/p/b88105bb404a
神经网络中Epoch、Iteration、Batchsize相关理解和说明 https://blog.csdn.net/program_developer/article/details/78597738
文本分类实践,TextCNN,实战 https://www.pythonf.cn/read/81023